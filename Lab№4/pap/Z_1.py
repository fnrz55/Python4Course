# 1. Загрузка данных и вывод на экран структуры данных.
# 2. Определите и опишите, какие данные и какие структуры содержаться в датасете(по ключу).
# 3. Задайте обучающие данные, обучающие метки, тестовые данные, тестовые метки с помощью функции train_test_split.
# 4. Выведите получившиеся обучающие и тестовые наборы данных, а также количество элементов в каждом наборе.
# 5. Визуализируйте данные с помощью диаграммы рассеяния.
# 6. Построение модели (k-ближайших соседей).
# 7. Получение прогноза.
# 8. Оценка качества модели.
import numpy as np
import pandas as pd
import mglearn
from matplotlib import pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier

iris_dataset = load_iris()

print("Ключи iris_dataset: \n {}".format(iris_dataset.keys()))
print("Описание iris_dataset: \n {}".format(iris_dataset['DESCR']))
# 'data' - 'данные' - Набор данных о растениях
# 'target' - 'цель' - Цели определяют количество значений выходных переменных.
# 'frame' - 'фрейм'
# 'target_names' - 'имена целевых объектов'
# 'DESCR' - 'ОПИСАНИЕ'
# Ключи iris_dataset:
#  .. _iris_dataset:
#
# Набор данных о растениях ириса
# --------------------
#
# **Характеристики набора данных:**
#
# :Количество экземпляров: 150 (по 50 в каждом из трех классов)
# :Количество атрибутов: 4 числовых, прогнозирующих атрибута и класс
# :Информация об атрибуте:
#     - длина чашелистика в см
#     - ширина чашелистика в см
#     - длина лепестка в см
#     - ширина лепестка в см
#     - класс:
#             - Ирис-Сетоза
# - Ирис разноцветный
#             - Ирис Виргинский
#
# :Сводная статистика:
# ============== ==== ==== ======= ===== ====================
#                 Min  Max   Mean    SD   Class Correlation
# ============== ==== ==== ======= ===== ====================
# sepal length:   4.3  7.9   5.84   0.83    0.7826
# sepal width:    2.0  4.4   3.05   0.43   -0.4194
# petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)
# petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)
# ============== ==== ==== ======= ===== ====================
#                 Минимальная Максимальная средняя корреляция классов SD
# ============== ==== ==== ======= ===== ====================
# длина чашелистика:   4,3 7,9 5,84 0,83 0,7826
# ширина чашелистика: 2,0 4,4 3,05 0,43 -0,4194
# длина лепестка:   1,0 6,9 3,76 1,76 0,9490 (высота!)
# ширина лепестка: 0,1 2,5 1,20 0,76 0,9565 (высота!)
# ============== ==== ==== ======= ===== ====================
#
# :Отсутствующие значения атрибутов: Отсутствуют
# :Распределение по классам: 33,3% для каждого из 3 классов.
# :Создатель: Р.А. Фишер
# :Спонсор: Майкл Маршалл (MARSHALL%PLU@io.arc.nasa.gov)
# :Дата: июль, 1988
#
# Знаменитая база данных Iris, впервые использованная сэром Р.А. Фишером. Набор данных взят
# из статьи Фишера. Обратите внимание, что он такой же, как в R, но не такой, как в UCI
# Репозиторий машинного обучения, в котором указаны две неверные точки данных.
#
# Это, пожалуй, самая известная база данных, которую можно найти в литературе по
# распознаванию образов.  Статья Фишера является классической в этой области, и
# на нее часто ссылаются по сей день.  (см., например, Duda & Hart).  Набор
# данных содержит 3 класса по 50 экземпляров в каждом, где каждый класс относится к определенному
# типу растений ириса.  Один класс линейно отделим от двух других;
# последние линейно не отделимы друг от друга.
#
# .. выпадающий список:: Ссылки
#
#   - Фишер Р.А. "Использование множественных измерений в таксономических задачах"
#     Ежегодник евгеники, 7, Часть II, 179-188 (1936); также в разделе "Вклад в
#     Математическая статистика" (Джон Уайли, Нью-Йорк, 1950).
#   - Дуда, Р.О. и Харт, П.Э. (1973) Классификация паттернов и анализ сцен.
#     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  Смотрите страницу 218.
#   - Дасарати Б.В. (1980) "Обнюхивание окрестностей: новая система"
#     Структура и правила классификации для распознавания частично открытых объектов.
#     Среды".  IEEE сделки по анализу рисунка и машины
#     Интеллект, Том. Пами-2, вып. 1, 67-71.
#   - Гейтс, В. Г. (1972) "Сокращение Правило Ближайшего Соседа".  IEEE сделки
#     по теории информации, май 1972, с. 431-433.
#   - Смотрите также: Труды MLC за 1988 год, с. 54-64.  Разработанная Чизманом и др.
#     концептуальная система кластеризации AUTOCLASS II обнаруживает в данных 3 класса.
#   - И многое, многое другое...
# 'feature_names' - 'имена объектов'
# 'filename' - 'имя файла'
# 'data_module - 'модуль данных'

x_train, x_test, y_train, y_test = train_test_split(
    iris_dataset['data'], iris_dataset['target'], random_state=0
)
print('Обучающие данные  - x_train \n{}'.format(x_train))
print('Количество элементов: {}'.format(x_train.shape))
print('Обучающие метки - x_test \n{}'.format(x_test))
print('Количество элементов: {}'.format(x_test.shape))
print('Тестовые данные - y_train \n{}'.format(y_train))
print('Количество элементов: {}'.format(y_train.shape))
print('Тестовые метки - y_test \n{}'.format(y_test))
print('Количество элементов: {}'.format(y_test.shape))

# Создаём dataframe из данных в масиве x_train
# Маркируем столбцы, используя строки в iris_dataset.feature_names
iris_dataframe = pd.DataFrame(x_train, columns=iris_dataset.feature_names)
# Создаём матрицу рассеяния из dataframe, цвет точек задаём с помощью y_train
grr = pd.plotting.scatter_matrix(iris_dataframe,
                        c = y_train,
                        figsize = (15, 15),
                        marker = 'o',
                        hist_kwds = {'bins': 20},
                        s = 60,
                        alpha = .8,
                        cmap = mglearn.cm3)
plt.show()

mass = []
mass2 = []
for i in range(1, 100):
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(x_train, y_train)
    x_new = np.array([[5, 2.9, 1, 0.2]])
    prediction = knn.predict(x_new)
    y_pred = knn.predict(x_test)
    mass.append(np.mean(y_pred == y_test))
    mass2.append(knn.score(x_test, y_test))

fig, ax = plt.subplots(figsize=(14, 8))
plt.scatter(range(1, 100), mass, c='orange')
plt.scatter(range(1, 100), mass2, c='blue')
plt.show()
print('Массив результатов \n{}'.format(mass))



# knn = KNeighborsClassifier(n_neighbors=1)
# print('Построение модели (k-ближайших соседей) \n{}'.format(knn.fit(x_train, y_train)))
#
#  
# print('Форма массива x_new: \n{}'.format(x_new.shape))
# prediction = knn.predict(x_new)
# print('Прогноз: \n{}'.format(prediction))
# print('Спрогнозированная метка: \n{}'.format(iris_dataset['target_names'][prediction]))
#
# y_pred = knn.predict(x_test)
# print('Прогнозы для тестового набора: \n{}'.format(y_pred))
#
# print('Правильность на тестовом наборе: \n{:.2f}'.format(np.mean(y_pred == y_test)))
# print('Правильность на тестовом наборе: \n{:.2f}'.format(knn.score(x_test, y_test)))